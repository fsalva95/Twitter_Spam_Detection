{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/balakesavan/nltk-library-for-common-word-and-tfidf-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "d185e5fb-ac9a-40a2-b563-d9aa1a77f94e",
    "_uuid": "1008fd2001c2b8485d2b4815813f90b722286c22"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import pickle \n",
    "#import mglearn\n",
    "import time\n",
    "\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer # doesn't split at apostrophes\n",
    "import nltk\n",
    "from nltk import Text\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e410d5e3-a909-4dde-9d13-d1963063469b",
    "_uuid": "5b534ccd8f7584c1e97adf86486664fb0450af58"
   },
   "source": [
    "# CountVectorizer -- Brief Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b7098871-bb8d-4460-9019-4fc61b4ebd33",
    "_uuid": "a7a2bd7c30e900fb6ad66091868a647670626490"
   },
   "source": [
    "*    CountVectorizer can lowercase letters, disregard punctuation and stopwords, but it can't LEMMATIZE or STEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "0394cd12-20e6-4143-bf06-c69b2bfe1406",
    "_uuid": "376ae829a98e2b2de88e51e099e71d5e0b10e190"
   },
   "outputs": [],
   "source": [
    "txt = [\"He is ::having a great Time, at the park time?\",\n",
    "       \"She, unlike most women, is a big player on the park's grass.\",\n",
    "       \"she can't be going\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b71fb093-b9ef-47e4-9e82-28cb6eaad710",
    "_uuid": "b6481fd89ec95f6766aa3895d2a6fe89686dce5b"
   },
   "source": [
    "**Features in Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "0fad19fd-db7f-4d75-bb72-93d9cea919a1",
    "_uuid": "0ae95c0f33c248326acea22e4f908d011e85124c",
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature:\n",
      "['big', 'going', 'grass', 'great', 'having', 'park', 'player', 'time', 'unlike', 'women']\n",
      "\n",
      "Every 3rd feature:\n",
      "['big', 'great', 'player', 'women']\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n",
    "\n",
    "#count_vec = CountVectorizer(stop_words=None, analyzer='word', \n",
    "#                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n",
    "\n",
    "# Transforms the data into a bag of words\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "# Print the first 10 features of the count_vec\n",
    "print(\"Every feature:\\n{}\".format(count_vec.get_feature_names()))\n",
    "print(\"\\nEvery 3rd feature:\\n{}\".format(count_vec.get_feature_names()[::3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she can't be going\n",
      "Every feature:\n",
      "['big', 'going', 'grass', 'great', 'having', 'park', 'player', 'time', 'unlike', 'women']\n",
      "[[0 0 0 1 1 1 0 2 0 0]\n",
      " [1 0 1 0 0 1 1 0 1 1]\n",
      " [0 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(txt[2])\n",
    "print(\"Every feature:\\n{}\".format(count_vec.get_feature_names()))\n",
    "print(bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "950bcc7b-4011-4a65-af4c-ca46e0ff1bc7",
    "_uuid": "e2e7d7569b95d531fa3960d0d452690a1e2e6151"
   },
   "source": [
    "**Vocabulary and vocabulary ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "730a0d62-b034-4a78-9e5a-906e0fefffa2",
    "_uuid": "ed2b7bc16a0c04c33f946661904dc95018c49235",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10\n",
      "Vocabulary content:\n",
      " {'having': 4, 'great': 3, 'time': 7, 'park': 5, 'unlike': 8, 'women': 9, 'big': 0, 'player': 6, 'grass': 2, 'going': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(count_train.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(count_train.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f4a4ad33-2cc2-4f38-b8f5-66c4cf9e449c",
    "_uuid": "ceaed2e0a255f2ab235b9694eb5115aaa7eb0796"
   },
   "source": [
    "# N-grams (sets of consecutive words)\n",
    "* N=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "3e233e9b-cd56-4f86-a977-e0fcb034b61d",
    "_uuid": "9aebf4daeec289943a79def5f7d1b777e495d606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'big player', 'going', 'grass', 'great', 'great time', 'having', 'having great', 'park', 'park grass', 'park time', 'player', 'player park', 'time', 'time park', 'unlike', 'unlike women', 'women', 'women big']\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 2), max_df=1.0, min_df=1, max_features=None)\n",
    "\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e39dc6fb-de14-4c5d-8768-8b0ebdce6a75",
    "_uuid": "6dcc6ce87193525b686a588019d36e2411e26a3a"
   },
   "source": [
    "* N=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "6d6d655b-b3d2-4a28-a435-cdf46c601cc5",
    "_uuid": "0d37a520725d1ee77cf57e251c3d80f0efa59ffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'big player', 'big player park', 'going', 'grass', 'great', 'great time', 'great time park', 'having', 'having great', 'having great time', 'park', 'park grass', 'park time', 'player', 'player park', 'player park grass', 'time', 'time park', 'time park time', 'unlike', 'unlike women', 'unlike women big', 'women', 'women big', 'women big player']\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 3), max_df=1.0, min_df=1, max_features=None)\n",
    "\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "772bca4c-7f5e-4002-92b3-3778af49e7b4",
    "_uuid": "3b591bc9413f1afb857620edb6cd6752b043f5d9"
   },
   "source": [
    "# Min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "175e2d4f-57a2-413d-9475-38a8cff0c9c9",
    "_uuid": "a5b3098a5250f15b5ab46ad69394155a665853bf"
   },
   "source": [
    "**Min_df ignores terms that have a document frequency (presence in % of documents) strictly lower than the given threshold. For example, Min_df=0.66 requires that a term appear in 66% of the docuemnts for it to be considered part of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "04bc030a-2065-44c9-b650-79d592ce1ff8",
    "_uuid": "e2cd00e3929caa6fbbd500d3ecbd29dc4966100a"
   },
   "source": [
    "**Sometimes min_df is used to limit the vocabulary size, so it learns only those terms that appear in at least 10%, 20%, etc. of the documents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "dc5dec8d-7c0c-4a1c-995c-12d47521ddad",
    "_uuid": "713d59da4bdb79c32047bd83758effcb2c21bb57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['park']\n",
      "\n",
      "Only 'park' becomes the vocabulary of the document term matrix (dtm) because it appears in 2 out of 3 documents, meaning 0.66% of the time.      \n",
      "The rest of the words such as 'big' appear only in 1 out of 3 documents, meaning 0.33%. which is why they don't appear\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=0.6, max_features=None)\n",
    "\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "print(count_vec.get_feature_names())\n",
    "print(\"\\nOnly 'park' becomes the vocabulary of the document term matrix (dtm) because it appears in 2 out of 3 documents, \\\n",
    "meaning 0.66% of the time.\\\n",
    "      \\nThe rest of the words such as 'big' appear only in 1 out of 3 documents, meaning 0.33%. which is why they don't appear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c585fd8c-c8d7-4135-8631-2cab061d2a21",
    "_uuid": "e50a5e259f7d1a41455c78454f94c2490852278e"
   },
   "source": [
    "# Max_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd77ec37-22bc-4c7c-b104-be9cdbc33b15",
    "_uuid": "2555d4de9fce90b612402b09bf994359297b95e0"
   },
   "source": [
    "**When building the vocabulary, it ignores terms that have a document frequency strictly higher than the given threshold. This could be used to exclude terms that are too frequent and are unlikely to help predict the label. For example, by analyzing reviews on the movie Lion King, the term 'Lion' might appear in 90% of the reviews (documents), in which case, we could consider establishing Max_df=0.89**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "bce42711-e450-406c-a863-4c50f0eb41bb",
    "_uuid": "f0916d8143e9d6538b87fb81c6d1b9c80452cb0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'going', 'grass', 'great', 'having', 'player', 'time', 'unlike', 'women']\n",
      "\n",
      "Only 'park' is ignored because it appears in 2 out of 3 documents, meaning 0.66% of the time.\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=0.50, min_df=1, max_features=None)\n",
    "\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "print(count_vec.get_feature_names())\n",
    "print(\"\\nOnly 'park' is ignored because it appears in 2 out of 3 documents, meaning 0.66% of the time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bba8aa26-d2c1-442e-9ecb-7f4b54bb8c96",
    "_uuid": "59a326fbe009ef970268b6432b5541f661703ee1"
   },
   "source": [
    "# Max_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4486c0c4-e316-4074-9c3c-05fbac40daac",
    "_uuid": "ad23dfaed3f75e0bce22669ceef43623a0c0825e"
   },
   "source": [
    "**Limit the amount of features (vocabulary) that the vectorizer will learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "65869d10-40fd-435d-bc5f-cc599208f37c",
    "_uuid": "3e376fb48a8fa72be1ccf76a48d52bcd593056fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'going', 'park', 'time']\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=4)\n",
    "\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d85e6ea-2f25-4f14-8cf5-0c454999f911",
    "_uuid": "2a7123450a186ae74521da4da847161fd92e9af3"
   },
   "source": [
    "# TfidfVectorizer -- Brief Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bbb398f7-2593-4412-9545-7db93628d334",
    "_uuid": "62c7c509333505e0173deb4ab7d27738cf940023"
   },
   "source": [
    "The goal of using tf-idf is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus. (https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/feature_extraction/text.py#L1365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1176ec41-b021-4524-9936-7c6c56e13f7b",
    "_uuid": "0cf8e9c74cebae1be6ce69b13fe0224c636155f4"
   },
   "source": [
    "formula used: \n",
    "tf-idf(d, t) = tf(t) * idf(d, t)\n",
    "                * tf(t)= the term frequency is the number of times the term appears in the document\n",
    "                * idf(d, t) = the document frequency is the number of documents 'd' that contain term 't'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "fd0b0b13-ab84-4771-a99d-74bd9819e7db",
    "_uuid": "7f5155fcce0056330a680368dea2161570783e1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text:  ['His smile was not perfect', 'His smile was not not not not perfect', 'she not sang']\n"
     ]
    }
   ],
   "source": [
    "txt1 = ['His smile was not perfect', 'His smile was not not not not perfect', 'she not sang']\n",
    "tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\n",
    "txt_fitted = tf.fit(txt1)\n",
    "txt_transformed = txt_fitted.transform(txt1)\n",
    "print (\"The text: \", txt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "25f000cf-02e1-4bb8-af0d-6c88828ab9f7",
    "_uuid": "279a982ce89664fcb87b6bb79bbc82c743d0deae"
   },
   "source": [
    "The learned corpus vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "35c2e277-6d95-4ae9-b338-d471053ba887",
    "_uuid": "9168c7438a90a71b0e8e988761682c5c0f05f10b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'his': 0, 'smile': 5, 'was': 6, 'not': 1, 'perfect': 2, 'she': 4, 'sang': 3}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "73e8356b-b2e1-4630-b8f7-cc4910d99dae",
    "_uuid": "d0b14b583014041ed8c04aee71737e4bbf47ffab"
   },
   "source": [
    "**IDF:** The inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "c3f5b1b9-29d8-451b-988e-c913f725c5ef",
    "_uuid": "ea54e8f36de0ba1d33670ea1b25f6e908fc55e48",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'his': 1.4054651081081644, 'not': 1.0, 'perfect': 1.4054651081081644, 'sang': 2.09861228866811, 'she': 2.09861228866811, 'smile': 1.4054651081081644, 'was': 1.4054651081081644}\n",
      "\n",
      "We see that the tokens 'sang','she' have the most idf weight because they are the only tokens that appear in one document only.\n",
      "\n",
      "The token 'not' appears 6 times but it is also in all documents, so its idf is the lowest\n"
     ]
    }
   ],
   "source": [
    "idf = tf.idf_\n",
    "print(dict(zip(txt_fitted.get_feature_names(), idf)))\n",
    "print(\"\\nWe see that the tokens 'sang','she' have the most idf weight because \\\n",
    "they are the only tokens that appear in one document only.\")\n",
    "print(\"\\nThe token 'not' appears 6 times but it is also in all documents, so its idf is the lowest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8170cef9-c18c-419a-9549-97f05e0cba6d",
    "_uuid": "3875bc2d774af0b145dae24d49cddce41061794e"
   },
   "source": [
    "Graphing inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "a688eebb-616e-447c-981e-8b4710355b55",
    "_uuid": "90107fb2986dca9bcfa54a117ce6e5f70175c156"
   },
   "outputs": [],
   "source": [
    "rr = dict(zip(txt_fitted.get_feature_names(), idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "5631f6f6-0e10-4ce0-8d41-bd2a740742cf",
    "_uuid": "2841ce8e683e2dc4995db669a1b24f0af851c074"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_weight = pd.DataFrame.from_dict(rr, orient='index').reset_index()\n",
    "token_weight.columns=('token','weight')\n",
    "token_weight = token_weight.sort_values(by='weight', ascending=False)\n",
    "token_weight \n",
    "\n",
    "sns.barplot(x='token', y='weight', data=token_weight)            \n",
    "plt.title(\"Inverse Document Frequency(idf) per token\")\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1780ee0-f805-4125-8854-adce357e4914",
    "_uuid": "e8de64851a2aba585aa11b711fd8919763de456b"
   },
   "source": [
    "Listing (instead of graphing) inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "5323f75b-0cb6-4880-b049-7fb974c270ab",
    "_uuid": "759cec0a7d85c2367affcccdb7f1c5f2a8b0ee4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest idf:\n",
      "['not' 'his' 'perfect']\n",
      "\n",
      "Features with highest idf:\n",
      "['was' 'sang' 'she']\n"
     ]
    }
   ],
   "source": [
    "# get feature names\n",
    "feature_names = np.array(tf.get_feature_names())\n",
    "sorted_by_idf = np.argsort(tf.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(\n",
    "       feature_names[sorted_by_idf[:3]]))\n",
    "print(\"\\nFeatures with highest idf:\\n{}\".format(\n",
    "       feature_names[sorted_by_idf[-3:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f34a935-2c22-4500-a551-97a810072426",
    "_uuid": "40bf8ce1fc5d50951716284de9c7cb5783dd8f8b"
   },
   "source": [
    "**Weight of tokens per document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "b2432306-2012-478e-b74a-ed80eebc7761",
    "_uuid": "253e04bf9807ea477b703a37205aa8a4609b73a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token 'not' has  the largest weight in document #2 because it appears 3 times there. But in document #1 its weight is 0 because it does not appear there.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.40546511, 1.        , 1.40546511, 0.        , 0.        ,\n",
       "        1.40546511, 1.40546511],\n",
       "       [1.40546511, 4.        , 1.40546511, 0.        , 0.        ,\n",
       "        1.40546511, 1.40546511],\n",
       "       [0.        , 1.        , 0.        , 2.09861229, 2.09861229,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The token 'not' has  the largest weight in document #2 because it appears 3 times there. But in document #1\\\n",
    " its weight is 0 because it does not appear there.\")\n",
    "txt_transformed.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "344187ac-114d-4629-b5a2-b7445edaccc8",
    "_uuid": "540fe13829199d236ce703b6b8039f29497792e6"
   },
   "source": [
    "* Summary: the more times a token appears in a document, the more weight it will have. However, the more documents the token appears in, it is 'penalized' and the weight is diminished. For example, the weight for token 'not' is 4, but if it did not appear in all documents (that is, only in one document) its weight would have been 8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f080fbb1-5367-4acb-9e1c-ab5b78414823",
    "_uuid": "3c24d989a7459b6287f685a88c076f2d2fa5955f"
   },
   "source": [
    "**TF-IDF** - Maximum token value throughout the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "a8fdc906-7b4c-4c61-a1ad-96bb006fa464",
    "_uuid": "5e4a087db6f509055227304ebf3e021f35b4496d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest tfidf:\n",
      "['his' 'perfect' 'smile']\n",
      "\n",
      "Features with highest tfidf: \n",
      "['sang' 'she' 'not']\n"
     ]
    }
   ],
   "source": [
    "new1 = tf.transform(txt1)\n",
    "\n",
    "# find maximum value for each of the features over all of dataset:\n",
    "max_val = new1.max(axis=0).toarray().ravel()\n",
    "\n",
    "#sort weights from smallest to biggest and extract their indices \n",
    "sort_by_tfidf = max_val.argsort()\n",
    "\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(\n",
    "      feature_names[sort_by_tfidf[:3]]))\n",
    "\n",
    "print(\"\\nFeatures with highest tfidf: \\n{}\".format(\n",
    "      feature_names[sort_by_tfidf[-3:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "51e80304-6daa-40a3-8465-b85766ebd4a9",
    "_uuid": "ef2392efce8759ec793d5f7a6e9e7614ca752e35"
   },
   "source": [
    "# Clean, Train, Vectorize, Classify Toxic Comments (w/o parameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a4826927-b947-4214-8f60-944f1075cf57",
    "_uuid": "f0e635033a2d73bd9d1806c9bace230ebcd5590d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "holdout = pd.read_csv('../input/test.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b6ee41ff-4474-474e-a459-39dfaa38a061",
    "_uuid": "742a77ed9d25e46005080410621b62a16538a2cd"
   },
   "source": [
    "**Clean Train text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "abdf3572-b9a3-45bb-b7a0-9b0c2fa35d4a",
    "_uuid": "9de549ce447f1333cd6ede6886added51ff3911e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Lemmatizing and stemming gives us a lower ROC-AUC score. So we will only clean \\\\n's, Username, IP and http links\"\"\"\n",
    "\n",
    "start_time=time.time()\n",
    "# remove '\\\\n'\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "    \n",
    "# remove any text starting with User... \n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n",
    "    \n",
    "# remove IP addresses or user IDs\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "    \n",
    "#remove http links in the text\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n",
    "\n",
    "end_time=time.time()\n",
    "print(\"total time\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "afa9406e-7ced-4d6e-bb32-3dfe7d3e577c",
    "_uuid": "df34f4291a3e31321c7815e177f17879df0c5b05"
   },
   "source": [
    "Cleaning HOLDOUT text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7f5c84fc-95d6-4701-be7d-e511a8e20691",
    "_uuid": "8830778f50bdf5ebf32fb27d1e91797b8a523cff",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove '\\\\n'\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "    \n",
    "# remove any text starting with User... \n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n",
    "    \n",
    "# remove IP addresses or user IDs\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "    \n",
    "#remove http links in the text\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a41847e9-6d11-4cb7-ba66-f2ec053667a6",
    "_uuid": "c96483ed085c7cd7e1e9a702e5ad6c0e67e90ae3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = train['comment_text']\n",
    "y = train.iloc[:, 2:8]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "de397109-2345-47d5-8e12-8b9c65d70358",
    "_uuid": "635071d49f39a5386e92ce0620accc77dab7ee6e"
   },
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "339d3dad-fb13-4b80-b6f8-148c44cbfb6a",
    "_uuid": "c65f49f9570cc3f26e25e2b1d96801de87e585b6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "83920f30-aed7-44f7-b90e-f006ba230eb2",
    "_uuid": "adf6f40983f64a449a813419cd88347d1a187ab1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aa20a27e-29f7-46ba-9b67-1b3b6a118af5",
    "_uuid": "1efad69e7076c4d842d8a242a33a4c57cd4f31e1"
   },
   "source": [
    "**Vectorize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e35dd40f-3586-42b6-808c-32697130d739",
    "_uuid": "cb676ae52ed56967028e77736590e47df984532e",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the vectorizer\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{2,}',  #vectorize 2-character words or more\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=30000)\n",
    "\n",
    "# fit and transform on it the training features\n",
    "word_vectorizer.fit(X_train)\n",
    "X_train_word_features = word_vectorizer.transform(X_train)\n",
    "\n",
    "#transform the test features to sparse matrix\n",
    "test_features = word_vectorizer.transform(X_test)\n",
    "\n",
    "# transform the holdout text for submission at the end\n",
    "holdout_text = holdout['comment_text']\n",
    "holdout_word_features = word_vectorizer.transform(holdout_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5ebf3032-9599-4d7f-9ab4-76167967e842",
    "_uuid": "9a3050e360332def7afbac5cb734d76e60348582"
   },
   "source": [
    "# Classify \n",
    "* Run a Logistic regression on each label separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "60620870-73b9-4625-9fa2-d4182b6b0b44",
    "_uuid": "9c085a03f4ddef9602211154e701eef39253e74f",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_names = ['toxic','severe_toxic','obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "losses = []\n",
    "auc = []\n",
    "\n",
    "for class_name in class_names:\n",
    "    #call the labels one column at a time so we can run the classifier on them\n",
    "    train_target = y_train[class_name]\n",
    "    test_target = y_test[class_name]\n",
    "    classifier = LogisticRegression(solver='sag', C=10)\n",
    "\n",
    "    cv_loss = np.mean(cross_val_score(classifier, X_train_word_features, train_target, cv=5, scoring='neg_log_loss'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV Log_loss score for class {} is {}'.format(class_name, cv_loss))\n",
    "\n",
    "    cv_score = np.mean(cross_val_score(classifier, X_train_word_features, train_target, cv=5, scoring='accuracy'))\n",
    "    print('CV Accuracy score for class {} is {}'.format(class_name, cv_score))\n",
    "    \n",
    "    classifier.fit(X_train_word_features, train_target)\n",
    "    y_pred = classifier.predict(test_features)\n",
    "    y_pred_prob = classifier.predict_proba(test_features)[:, 1]\n",
    "    auc_score = metrics.roc_auc_score(test_target, y_pred_prob)\n",
    "    auc.append(auc_score)\n",
    "    print(\"CV ROC_AUC score {}\\n\".format(auc_score))\n",
    "    \n",
    "    print(confusion_matrix(test_target, y_pred))\n",
    "    print(classification_report(test_target, y_pred))\n",
    "\n",
    "print('Total average CV Log_loss score is {}'.format(np.mean(losses)))\n",
    "print('Total average CV ROC_AUC score is {}'.format(np.mean(auc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "72e16659-bbc3-4741-b7f4-f2deeeec0d09",
    "_uuid": "4fd9be727670c7d43003128c70b3f4901112d310"
   },
   "source": [
    "# Vectorize, Classify (with parameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6ca01622-bf03-4d11-9298-dff126482f5c",
    "_uuid": "f2ea2edf3f2bed3948ab7d8bc2dd3f9c15474047",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = train['comment_text']\n",
    "y = train.iloc[:, 2:8]  \n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "343a5a48-fc82-42e9-92d1-ac35926d45a4",
    "_uuid": "e585b03c4e006b78708a04d391b68df808b1c8b8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time=time.time()\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(\n",
    "                                    stop_words='english',\n",
    "                                    strip_accents='unicode',\n",
    "                                    token_pattern=r'\\w{1,}', #accept tokens that have 1 or more characters\n",
    "                                    analyzer='word',\n",
    "                                    ngram_range=(1, 1),\n",
    "                                    min_df=5),\n",
    "                     OneVsRestClassifier(LogisticRegression()))\n",
    "param_grid = {'tfidfvectorizer__max_features': [10000, 30000],\n",
    "              'onevsrestclassifier__estimator__solver': ['liblinear', 'sag'],\n",
    "             } \n",
    "grid = GridSearchCV(pipe, param_grid, cv=3, scoring='roc_auc')\n",
    "\n",
    "grid3 = grid.fit(X_train, y_train)\n",
    "\n",
    "end_time=time.time()\n",
    "print(\"total time\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7dfd47ff-7a31-49d5-b299-e3eae6a5060a",
    "_uuid": "8f04589b851aa36b7cdeaab8012ba413e56343cc"
   },
   "source": [
    "# Pickle the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "365908a6-908f-4646-bd7c-8a24ab499cc5",
    "_uuid": "13db20ef0650d8ad58b48ab4060d68d1400c959f"
   },
   "source": [
    "Use Pickle to save files, documents, trained algorithms, etc., on your computer. In this case, we are saving our PC's processor the time it would take it to fit and transform all the text and run a logistic regression(345 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "020649de-0f22-4d0d-b603-9230f3853331",
    "_uuid": "8f31c41a6182446c35ec8da89047b4c2fd634af7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save classifier to a file\n",
    "\n",
    "# save_classifier = open(\"Tfidf_LogR_3.pickle\", 'wb') #wb= write in bytes. \n",
    "# pickle.dump(grid3, save_classifier) #use pickle to dump the grid3 we trained, as 'Tfidf_LogR.pickle' in wb format\n",
    "# save_classifier.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a7f2d38a-39e1-4892-a35b-8117b48a43c8",
    "_uuid": "dfacdd4e79f97852fcf0906fe5e941dd6f49cb6c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the saved file and uplaod it to an object\n",
    "\n",
    "# vec = open(\"Tfidf_LogR_3.pickle\", 'rb') # rb= read in bytes\n",
    "# grid3 = pickle.load(vec)\n",
    "# vec.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c96fc18-a711-4265-9c5a-52cf2d5c44a8",
    "_uuid": "8875d04b5196b1e77041b0e0bcb4c23c52bb1524"
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6316d25d-06af-43f3-ac6b-ea03ec647da8",
    "_uuid": "d298f725272b2cf2040b781588aa161ac14f9efb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(grid3.best_estimator_.named_steps['onevsrestclassifier'])\n",
    "print(grid3.best_estimator_.named_steps['tfidfvectorizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3018d946-7da8-4ef0-9337-ee4f7bcf7763",
    "_uuid": "e5e09eaa06c85b047725d3525c0e4d0c8a770470",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d9eba92d-4ee0-45ea-a486-0bb218ca2ea0",
    "_uuid": "8d45945a3e6279bb0717165e3de0824eb0a68688",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1ee9fe39-fc90-4922-a584-0febcbf84fe9",
    "_uuid": "5ba81ef9e107ea0a9f9d91ee7c0ebcbc347773a9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_y_test = grid3.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d0a18843-1ee3-4736-bc8a-824075ee25cc",
    "_uuid": "56174172c8e735c880a7eef2279a5124082743c9"
   },
   "source": [
    "We see that our recall is the lowest with severely toxic, threats, and identity_ hate comments. Perhaps if we had a higher number of comments (more data) in those categories, our classifier would do better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2542b207-69b7-4f3b-a7cf-62c29bdecd88",
    "_kg_hide-input": true,
    "_uuid": "8eb01bd6ad33c515c2dd0f76824ba14e434b6046",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Toxic Confusion Matrixs: \\n{}\".format(confusion_matrix(y_test['toxic'], predicted_y_test[:,0])))\n",
    "print(\"\\nSevere Toxic: \\n{}\".format(confusion_matrix(y_test['severe_toxic'], predicted_y_test[:,1])))\n",
    "print(\"\\nObscene: \\n{}\".format(confusion_matrix(y_test['obscene'], predicted_y_test[:,2])))\n",
    "print(\"\\nThreat: \\n{}\".format(confusion_matrix(y_test['threat'], predicted_y_test[:,3])))\n",
    "print(\"\\nInsult: \\n{}\".format(confusion_matrix(y_test['insult'], predicted_y_test[:,4])))\n",
    "print(\"\\nIdentity Hate: \\n{}\".format(confusion_matrix(y_test['identity_hate'], predicted_y_test[:,5])))\n",
    "\n",
    "print(\"\\nToxic Classification report: \\n{}\".format(classification_report(y_test['toxic'], predicted_y_test[:,0])))\n",
    "print(\"\\nSevere Toxic: \\n{}\".format(classification_report(y_test['severe_toxic'], predicted_y_test[:,1])))\n",
    "print(\"\\nObscene: \\n{}\".format(classification_report(y_test['obscene'], predicted_y_test[:,2])))\n",
    "print(\"\\nThreat: \\n{}\".format(classification_report(y_test['threat'], predicted_y_test[:,3])))\n",
    "print(\"\\nInsult: \\n{}\".format(classification_report(y_test['insult'], predicted_y_test[:,4])))\n",
    "print(\"\\nIdentity Hate: \\n{}\".format(classification_report(y_test['identity_hate'], predicted_y_test[:,5])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3aae349b-4f8e-4fa7-b48b-17f27f5b115b",
    "_uuid": "adabe2efb8ebe53fa9264a030963cc96195ee0fe",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grid3.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "da072ea2-cb69-4ecf-9de6-f86afcba915a",
    "_uuid": "85b71dd0a9a952f0b80444315129bbccddd05537",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = grid3.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "# transform the training dataset:\n",
    "X_test_set = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# find maximum value for each of the features over dataset:\n",
    "max_value = X_test_set.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "\n",
    "# get feature names\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(\n",
    "      feature_names[sorted_by_tfidf[:20]]))\n",
    "\n",
    "print(\"\\nFeatures with highest tfidf: \\n{}\".format(\n",
    "      feature_names[sorted_by_tfidf[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c4aa613b-930d-4ef0-b572-765ad88d5453",
    "_uuid": "dbccbb2972a1ba3f0759a49fa444e11141727964",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(\n",
    "       feature_names[sorted_by_idf[:100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7c0b3451-f3e4-4142-85d6-55f5b48a280e",
    "_uuid": "3c3ace54bda8bc3e293943eddd4d3b7bda9f97fd"
   },
   "source": [
    "# Graphing coefficients of tokens in toxic comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fbcdc03c-4ddb-4862-bb7f-8b0f03e2906d",
    "_uuid": "a36047c2a2caa7fa45ae000a7caee3fad21bef75"
   },
   "source": [
    "This would work only once you downlaod the mglearn library, as it does not exist on Kaggle. Many thanks to Andreas Mueller. This is his work and code: https://github.com/amueller/introduction_to_ml_with_python/blob/master/07-working-with-text-data.ipynb \n",
    "* Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "73b72d34-d328-4632-9e36-d8c4e5cb0ec9",
    "_uuid": "5cae2dd7f82a8f84325d46f76ac478965ba6b5aa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(y_train.columns)\n",
    "# print(\"\\n-Columns are ordered as above, which is why coef_[0] refers to toxic and coef_[5] refers to identity hate.\")\n",
    "# print(\"-The blue bars refer to the label (toxic here) and the red refer to Not toxic\")\n",
    "# mglearn.tools.visualize_coefficients(\n",
    "#     grid3.best_estimator_.named_steps[\"onevsrestclassifier\"].coef_[0],\n",
    "#     feature_names, n_top_features=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "42aa3160-eac5-41bc-a9f3-cd3560cf138d",
    "_uuid": "326cf0bda1d775fb86c7661df6b4e30968e2a550"
   },
   "source": [
    "* Severe toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6cbac347-67eb-4157-b4b2-dcf37542ebd1",
    "_uuid": "68b06b6ca204950d19041a234b25ef492f864e06",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mglearn.tools.visualize_coefficients(\n",
    "#     grid3.best_estimator_.named_steps[\"onevsrestclassifier\"].coef_[1],\n",
    "#     feature_names, n_top_features=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "10bdb06e-94de-4ab9-8c21-4aeb4825cbea",
    "_uuid": "ff809cb56399962c3c008c89d943ce1e1f6c7d80"
   },
   "source": [
    "* Identity Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c8a235b7-d4b6-4e9b-bf80-23c9f015fa6e",
    "_uuid": "d22ef7b079fb59344e40e47e1e7a2b70d585c404",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mglearn.tools.visualize_coefficients(\n",
    "#     grid3.best_estimator_.named_steps[\"onevsrestclassifier\"].coef_[5],\n",
    "#     feature_names, n_top_features=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e9f94f3d-d249-461b-88a8-c222d2b2985f",
    "_uuid": "b4018f7ecc6d1afffc059f6268d90acdba756b5a"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1a50997f-abf4-48c0-9573-2f9f76f30e78",
    "_uuid": "194263d5eea7fd36fe1dd5084326dce81fe37c5f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holdout_comments = holdout['comment_text']\n",
    "# holdoutComments are automatically transformed throguh the grid3 pipeline before prodicting probabilities\n",
    "twod = grid3.predict_proba(holdout_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bc1a0791-3638-44a3-acab-bde89c4cdbaa",
    "_uuid": "19acdb3a99625a7005ccfcf279a60ba92b97b6ff",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "holdout_predictions = {}\n",
    "holdout_predictions = {'id': holdout['id']}  \n",
    "\n",
    "holdout_predictions['toxic']=twod[:,0]\n",
    "holdout_predictions['severe_toxic']=twod[:,1]\n",
    "holdout_predictions['obscene']=twod[:,2]\n",
    "holdout_predictions['threat']=twod[:,3]\n",
    "holdout_predictions['insult']=twod[:,4]\n",
    "holdout_predictions['identity_hate']=twod[:,5]\n",
    "    \n",
    "submission = pd.DataFrame.from_dict(holdout_predictions)\n",
    "submission = submission[['id','toxic','severe_toxic','obscene','threat','insult','identity_hate']] #rearrange columns\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "07bd5052-c7a3-40f5-b24c-9582b2a6c22a",
    "_uuid": "3827aa89950c44c2e6de7da50ccec951fac8d981"
   },
   "source": [
    "# Bonus: Adding features to pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5679afdc-03d4-4307-9258-935923b40fe9",
    "_uuid": "f21414db6356577c5bb543725a09c7ff92979d9c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate lenght of characters in each comment\n",
    "train['len_character'] = train['comment_text'].apply(lambda x: len(re.findall(r\"[\\w]\", str(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0edba497-6aa4-43bc-815b-4ea4f9dfc4f1",
    "_uuid": "73cb91202a0c5ae3c393e96b53152ad2ed880c15",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion #unites all arrays into one array\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8e40940d-0276-4739-86e0-648bffbc98d1",
    "_uuid": "fdae5c022a7f5b6a2fe8d01b2da3464d9b8a0c56",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = train[['comment_text','len_character']] #these will be our features\n",
    "y = train.iloc[:, 2:8]  \n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9ec9900b-81aa-487b-adb3-351f0c0a466a",
    "_uuid": "c22b6896d287dbe339582cfcd1d8b3ac441fe1fe"
   },
   "source": [
    "Divide features into numeric and text features, so we can feed into the pipeline later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3af12de8-9b91-48ad-a26a-9a8d7ecf4862",
    "_uuid": "7c20bbb4a1edc48b048751095d6f330ecc8a6fdb",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(lambda a: a[['len_character']], validate=False)\n",
    "get_text_data = FunctionTransformer(lambda a: a['comment_text'], validate=False)\n",
    "\n",
    "print(get_text_data.fit_transform(X_train).shape)\n",
    "print(get_numeric_data.fit_transform(X_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "008ebf4f-af2e-4ba1-8617-6084b80386ab",
    "_uuid": "a9da1fb902047730c44f450f3f9f9a240b911436",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(                      #unites both text and numeric arrays into one array\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data)\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', TfidfVectorizer(\n",
    "                                                    stop_words='english',\n",
    "                                                    strip_accents='unicode',\n",
    "                                                    token_pattern=r'\\w{2,}',\n",
    "                                                    analyzer='word',\n",
    "                                                    ngram_range=(1, 1),\n",
    "                                                    min_df=5))\n",
    "                ]))\n",
    "             ]\n",
    "        )), #right here is where we would put interaction terms preprocessing such as PolynomialFeatures\n",
    "            #(right here is where we would put a scaler if we needed one)\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression())) \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2375bd17-04b7-4fb2-ad46-53a5aef8ab39",
    "_uuid": "efaebab6cd8fe34e4bb9761518dfb7167a708316",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'union__text_features__vectorizer__max_features': [10000, 30000],\n",
    "              'clf__estimator__C': [0.1, 1]\n",
    "             } \n",
    "grid = GridSearchCV(pl, param_grid, cv=3, scoring='roc_auc')\n",
    "\n",
    "grid4 = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "11608867-0086-4b96-acaa-c562f4abf1ed",
    "_uuid": "2592645b546e0aa597b07f348de55fcaa24a64a1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Pickle grid4 to your computer\n",
    "#dill: this is necessary in order for pickle to save grid4 which has a lambda function inside of it.\n",
    "import dill as pickled\n",
    "\n",
    "# save_grid4 = open(\"Tfidf_LogR_4.pickle\", 'wb') #wb= write in bytes. 'Tfidf_LogR.pickle' is the name of the file saved\n",
    "# pickled.dump(grid4, save_grid4) #use pickle to dump the grid1 we trained as 'Tfidf_LogR.pickle' in wb format\n",
    "# save_grid4.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "24be203d-7e87-4a33-b4ba-e713be192192",
    "_uuid": "1c8f629656a4bbb8a27a3b4b115cf15a793bbde1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill as pickled\n",
    "# Retrieve the saved file and uplaod it to an object\n",
    "\n",
    "# vec4 = open(\"Tfidf_LogR_4.pickle\", 'rb') # rb= read in bytes\n",
    "# grid4 = pickled.load(vec4)\n",
    "# vec4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8981421d-92c1-4922-9c8b-cece5307c2b8",
    "_uuid": "0e388a857486a67241a6e6cc84f2f2032d46c743",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(grid4.best_score_)\n",
    "print(grid4.best_params_)\n",
    "print(grid4.estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "22613839-6625-4a95-95bb-3a3ceca89b8b",
    "_uuid": "d5cfd4a4e9ca228e1ac0f64b0b5b6e78e628d39c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_y_test = grid4.predict(X_test)\n",
    "\n",
    "print(\"Toxic Confusion Matrixs: \\n{}\".format(confusion_matrix(y_test['toxic'], pred_y_test[:,0])))\n",
    "print(\"\\nSevere Toxic: \\n{}\".format(confusion_matrix(y_test['severe_toxic'], pred_y_test[:,1])))\n",
    "print(\"\\nObscene: \\n{}\".format(confusion_matrix(y_test['obscene'], pred_y_test[:,2])))\n",
    "print(\"\\nThreat: \\n{}\".format(confusion_matrix(y_test['threat'], pred_y_test[:,3])))\n",
    "print(\"\\nInsult: \\n{}\".format(confusion_matrix(y_test['insult'], pred_y_test[:,4])))\n",
    "print(\"\\nIdentity Hate: \\n{}\".format(confusion_matrix(y_test['identity_hate'], pred_y_test[:,5])))\n",
    "\n",
    "print(\"\\nToxic Classification report: \\n{}\".format(classification_report(y_test['toxic'], pred_y_test[:,0])))\n",
    "print(\"\\nSevere Toxic: \\n{}\".format(classification_report(y_test['severe_toxic'], pred_y_test[:,1])))\n",
    "print(\"\\nObscene: \\n{}\".format(classification_report(y_test['obscene'], pred_y_test[:,2])))\n",
    "print(\"\\nThreat: \\n{}\".format(classification_report(y_test['threat'], pred_y_test[:,3])))\n",
    "print(\"\\nInsult: \\n{}\".format(classification_report(y_test['insult'], pred_y_test[:,4])))\n",
    "print(\"\\nIdentity Hate: \\n{}\".format(classification_report(y_test['identity_hate'], pred_y_test[:,5])))\n"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
